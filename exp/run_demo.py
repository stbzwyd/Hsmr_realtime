from lib.kits.hsmr_demo import *
import cv2
import numpy as np
from pathlib import Path
import torch
import os

# å¯ç”¨ PyTorch å†…å­˜ä¼˜åŒ–
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

def main():
    # â›©ï¸ 0. Preparation.
    args = parse_args()
    outputs_root = Path(args.output_path)
    outputs_root.mkdir(parents=True, exist_ok=True)

    monitor = TimeMonitor()

    # â›©ï¸ 1. Preprocess.
    with monitor('Data Preprocess'):
        with monitor('Load Inputs'):
            raw_imgs, inputs_meta = load_inputs(args)

        with monitor('Detector Initialization'):
            get_logger(brief=True).info('ğŸ§± Building detector.')
            detector = build_detector(
                batch_size=args.det_bs,
                max_img_size=args.det_mis,
                device=args.device,
            )
            torch.cuda.empty_cache()  # é‡Šæ”¾æ˜¾å­˜

    # â›©ï¸ 2. Process inputs.
    if inputs_meta['type'] in ['camera', 'stream']:
        # å®æ—¶å¤„ç†ï¼šé€å¸§å¤„ç†
        cv2.namedWindow('HSMR Demo', cv2.WINDOW_NORMAL)
        frame_idx = 0
        save_video_flag = True
        video_writer = None
        frame_width, frame_height = None, None

        # åˆå§‹åŒ–æ¨ç†ç®¡é“
        with monitor('Pipeline Initialization'):
            get_logger(brief=True).info(f'ğŸ§± Building recovery pipeline.')
            pipeline = build_inference_pipeline(model_root=args.model_root, device=args.device)
            torch.cuda.empty_cache()  # é‡Šæ”¾æ˜¾å­˜

        for frame in raw_imgs:
            with monitor('Detecting'):
                frame_list = [frame]
                detector_outputs = detector(frame_list)
                patches, det_meta = imgs_det2patches(
                    frame_list, *detector_outputs, args.max_instances
                )
                torch.cuda.empty_cache()  # é‡Šæ”¾æ˜¾å­˜

            with monitor('Recovery'):
                if len(patches) == 0:
                    get_logger(brief=True).warning(f'No human instance detected in frame {frame_idx}.')
                    result_frame = frame
                else:
                    get_logger(brief=True).info(f'ğŸ” {len(patches)} human instances detected in frame {frame_idx}.')
                    patches_i = np.concatenate(patches, axis=0)
                    patches_normalized_i = (patches_i - IMG_MEAN_255) / IMG_STD_255
                    patches_normalized_i = patches_normalized_i.transpose(0, 3, 1, 2)
                    with torch.no_grad():
                        outputs = pipeline(patches_normalized_i)
                    pd_params = {k: v.detach().cpu().clone() for k, v in outputs['pd_params'].items()}
                    pd_cam_t = outputs['pd_cam_t'].detach().cpu().clone()

                    get_logger(brief=True).info(f'ğŸ¤Œ Preparing meshes...')
                    m_skin, m_skel = prepare_mesh(pipeline, pd_params)

                    with monitor('Visualization'):
                        m_skel = None if args.ignore_skel else m_skel
                        results, full_cam_t = visualize_full_img(
                            pd_cam_t, frame_list, det_meta, m_skin, m_skel, args.have_caption
                        )
                        result_frame = results[0]

            # æ˜¾ç¤ºç»“æœ
            result_frame_bgr = cv2.cvtColor(result_frame, cv2.COLOR_RGB2BGR)
            cv2.imshow('HSMR Demo', result_frame_bgr)

            # ä¿å­˜è§†é¢‘
            if save_video_flag:
                if video_writer is None:
                    frame_height, frame_width = result_frame_bgr.shape[:2]
                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                    video_path = outputs_root / f'{pipeline.name}-{inputs_meta["seq_name"]}.mp4'
                    video_writer = cv2.VideoWriter(str(video_path), fourcc, 30, (frame_width, frame_height))
                video_writer.write(result_frame_bgr)

            # æŒ‰ 'q' é€€å‡º
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

            frame_idx += 1

        # æ¸…ç†
        if video_writer:
            video_writer.release()
        cv2.destroyAllWindows()

    else:
        # é™æ€å›¾åƒæˆ–è§†é¢‘å¤„ç†ï¼ˆåŸå§‹é€»è¾‘ï¼‰
        with monitor('Detecting'):
            get_logger(brief=True).info(f'ğŸ–¼ï¸ Detecting...')
            detector_outputs = detector(raw_imgs)
            torch.cuda.empty_cache()  # é‡Šæ”¾æ˜¾å­˜

        with monitor('Patching & Loading'):
            patches, det_meta = imgs_det2patches(raw_imgs, *detector_outputs, args.max_instances)
            if len(patches) == 0:
                get_logger(brief=True).error(f'ğŸš« No human instance detected. Please ensure the validity of your inputs!')
                return
            get_logger(brief=True).info(f'ğŸ” Totally {len(patches)} human instances are detected.')

        # åˆå§‹åŒ–æ¨ç†ç®¡é“ï¼ˆå»¶è¿Ÿåˆ°æ£€æµ‹åï¼‰
        with monitor('Pipeline Initialization'):
            get_logger(brief=True).info(f'ğŸ§± Building recovery pipeline.')
            pipeline = build_inference_pipeline(model_root=args.model_root, device=args.device)
            torch.cuda.empty_cache()  # é‡Šæ”¾æ˜¾å­˜

        with monitor('Recovery'):
            get_logger(brief=True).info(f'ğŸƒ Recovering with B={args.rec_bs}...')
            pd_params, pd_cam_t = [], []
            for bw in asb(total=len(patches), bs_scope=args.rec_bs, enable_tqdm=True):
                patches_i = np.concatenate(patches[bw.sid:bw.eid], axis=0)
                patches_normalized_i = (patches_i - IMG_MEAN_255) / IMG_STD_255
                patches_normalized_i = patches_normalized_i.transpose(0, 3, 1, 2)
                with torch.no_grad():
                    outputs = pipeline(patches_normalized_i)
                pd_params.append({k: v.detach().cpu().clone() for k, v in outputs['pd_params'].items()})
                pd_cam_t.append(outputs['pd_cam_t'].detach().cpu().clone())

            pd_params = assemble_dict(pd_params, expand_dim=False)
            pd_cam_t = torch.cat(pd_cam_t, dim=0)
            dump_results = {
                'patch_cam_t': pd_cam_t.numpy(),
                **{k: v.numpy() for k, v in pd_params.items()},
            }

            get_logger(brief=True).info(f'ğŸ¤Œ Preparing meshes...')
            m_skin, m_skel = prepare_mesh(pipeline, pd_params)
            get_logger(brief=True).info(f'ğŸ Done.')

        with monitor('Visualization'):
            if args.ignore_skel:
                m_skel = None
            results, full_cam_t = visualize_full_img(pd_cam_t, raw_imgs, det_meta, m_skin, m_skel, args.have_caption)
            dump_results['full_cam_t'] = full_cam_t
            # Save rendering and dump results.
            if inputs_meta['type'] == 'video':
                seq_name = f'{pipeline.name}-' + inputs_meta['seq_name']
                save_video(results, outputs_root / f'{seq_name}.mp4')
                np.savez(outputs_root / f'{seq_name}.npz', **dump_results)
            elif inputs_meta['type'] == 'imgs':
                img_names = [f'{pipeline.name}-{fn.name}' for fn in inputs_meta['img_fns']]
                dump_results = disassemble_dict(dump_results, keep_dim=True)
                for i, img_name in enumerate(tqdm(img_names, desc='Saving images')):
                    save_img(results[i], outputs_root / f'{img_name}.jpg')
                    np.savez(outputs_root / f'{img_name}.npz', **dump_results[i])

            get_logger(brief=True).info(f'ğŸ¨ Rendering results are under {outputs_root}.')

    get_logger(brief=True).info(f'ğŸŠ Everything is done!')
    monitor.report()


if __name__ == '__main__':
    main()